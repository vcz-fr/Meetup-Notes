# Cloud Native Computing Foundation Bordeaux #5
üïë *Estimated reading time:* **?mn**

## Table of Contents

## CNCF time
- [Rook](https://rook.io/), the stateful storage solution for Kubernetes is now in v1.0. [Blog post](https://blog.rook.io/rook-v1-0-a-major-milestone-689ca4c75508);
- [Helm](https://helm.sh/) v3 will be announced in a Kubecon later this year. The major feature is the drop of Tiller;
- Speaking of Kubecon, the talks for the [Kubecon Europe 2019](https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/), set in Barcelona are live. Go watch them [here](https://www.youtube.com/playlist?list=PLj6h78yzYM2PpmMAnvpvsnR4c27wJePh3)!
- The next Kubecons will take place in Shanga√Ø from June 24th to 26th, then in San Diego from November 18th to 21st.

## Kubernetes at diet
By [Vincent Rabah](https://twitter.com/itwars), Digital Transformation / Innovation Director @ U-Need  
[Slides](https://www.it-wars.com/k3s.pdf) - [Blog post](https://www.it-wars.com/posts/cloud-native/kubernetes-avec-k3s-pour-sauver-la-planete/) - [Personal website](https://www.it-wars.com/) - [CV](https://www.it-wars.com/cv-vincent-rabah.pdf)

### Speaker notes

- [K3s repository](https://github.com/rancher/k3s/)
- [Ansible K3s repository](https://github.com/itwars/k3s-ansible)

- Good reads:
  - [The Copenhagen Catalog: 150 principles for a new direction in tech](https://www.copenhagencatalog.org/)
  - [Kubernetes Future: VMs, Containers, or Hypervisor?](https://www.infoq.com/news/2019/05/kubernetes-future/)
  - [Go memory ballast](https://blog.twitch.tv/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap-26c2462549a2)

- Projects:
  - [NexClipper: K8s monitoring](https://github.com/NexClipper/NexClipper)
  - [Polaris: Best practices validation for K8s clusters](https://github.com/reactiveops/polaris)
  - [K8dash: K8s real time dashboard](https://github.com/herbrandson/k8dash)
  - [Konstellate: K8s applications visualizer](https://github.com/containership/konstellate)
  - [Kubemove: Workload movement facilitation](https://github.com/kubemove/kubemove)
  - [OPS: Create and run Nanos unikernels](https://github.com/nanovms/ops)

### K8s vs K3s

Kubernetes is very complicated and that is okay. Even though it is present mostly everywhere today, most of its capacity remains unused. In order for Kubernetes to reach further, Rancher Labs built a lightweight version of it called [K3s](https://k3s.io/). With requirements of 200MB of storage and 512MB of RAM for the master and 75MB of RAM for each node, K3s is the perfect solution for resource-constrained environments. Not only that but it requires slightly less CPU resources than its fully featured counterpart.

From vanilla Kubernetes, K3s removes legacy and alpha features, most of the plugins (storage, cloud providers) and replaces [etcd](https://etcd.io/) with [SQLite](https://www.sqlite.org/), Docker with [containerd](https://containerd.io/) and uses [CoreDNS](https://coredns.io/) for service discovery, [Flannel](https://github.com/coreos/flannel) as the network fabric, [Traefik](https://traefik.io/) as Ingress Controller and [Helm](https://helm.sh/).

### K3s ecosystem

With Air Gap, you can also run K3s with no internet access! With this feature, you could store images as tar.gz files and embed them with your portable K3s solution.

The next step of the way for K3s is its purpose-built OS, [K3OS](https://k3os.io/), which will bridge K3s and the underlying Linux distribution together. It is currently based on a combination of Alpine and Ubuntu. It is still heavy and could gain from optimization work and current progress on [Unikernels](http://unikernel.org/).

If you wish to easily run K3s within Docker to try it out, you can do so with [K3d](https://github.com/rancher/k3d).

Big cloud vendors seem to be choosing containerd instead of Docker for their workloads today. The exact reason is unknown but we can suppose that it may be related to the fact that containerd being Docker without overhead, it initializes and attaches volumes faster.

Helm comes preinstalled without Tiller. You can still preload charts and execute them when the cluster initializes.

## Increase the reliability of production releases with Kubernetes
By [√âtienne Coutaud](https://twitter.com/etiennecoutaud), Founder @ Pyxida  
[Slides](https://speakerdeck.com/etiennecoutaud/fiabiliser-ses-deploiements-sur-kubernetes)

A good release should be:
- Invisible: No one notices that a release occurred, apart from the obvious changes;
- Painless: Minimal to no breakage;
- Mastered: The release process is automated and known to everyone;
- Chosen: The release decision should come from the development teams, not the deciders.

### Classic Kubernetes deployment

Inside Kubernetes, the classic workload organization places Pods inside ReplicaSets, which in turn are inside Deployments. The IngressController integrates the Services which direct the requests towards the pods they match.

Deploying is as simple as running the following command:
`kubectl set image deployment/your-workload containername=newimage:newtag`  
This will start a rolling update:
- A new ReplicaSet is created with the old one still running in parallel;
- If the new ReplicaSet is healthy, the traffic will be migrated over the new version.

It goes without saying: Liveness and Readiness probes must be implemented to help Kubernetes if, from the domain perspective, the pod is alive and running. Kubernetes supports HTTP, TCP and shell scripts to check the probes.  
‚ö†Ô∏è If a pod cannot start fast enough, it will be automatically restarted by Kubernetes. To avoid that, configure the `initialDelaySeconds` variable. [The official documentation](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/) hints at other solutions, depending on your workload.

This mechanism is very simple, keeps the latest ReplicaSets alive, is Zero-Downtime compliant and transparent for Ops. Nevertheless, nothing is tested during the deployment. Indeed, something that works on your staging environment may not survive when running at production scale.

### Blue / Green deployment

